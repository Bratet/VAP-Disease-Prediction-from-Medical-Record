{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_m.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3558 entries, 0 to 3557\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   VAP     3558 non-null   int64 \n",
      " 1   TEXT    3558 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 83.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pie chart of Label\n",
    "df['VAP'].value_counts().plot.pie(autopct='%1.1f%%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3558/3558 [00:13<00:00, 257.67it/s]\n"
     ]
    }
   ],
   "source": [
    "def parse_medical_records(text):\n",
    "\n",
    "    pattern = r'(\\[\\*\\*\\d{4}-\\d{1,2}-\\d{1,2}\\*\\*\\]\\s*\\d{1,2}:\\d{2}\\s*(AM|PM).*?(?=\\[\\*\\*\\d{4}-\\d{1,2}-\\d{1,2}\\*\\*\\]\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)|$))'\n",
    "    reports = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    extracted_reports = []\n",
    "    seen_reports = set()\n",
    "\n",
    "    for report_tuple in reports:\n",
    "        report = report_tuple[0]  # Extract the report from the tuple\n",
    "\n",
    "        # Extracting various pieces of information using regular expressions\n",
    "        report_id = re.search(r'\\[\\*\\*(\\d{4}-\\d{1,2}-\\d{1,2})\\*\\*\\]', report)\n",
    "        type_of_exam = re.search(r'CHEST \\((.*?)\\)', report)\n",
    "        clip_number = re.search(r'Clip Number \\(Radiology\\) (\\d+)', report)\n",
    "        reason_for_exam = re.search(r'Reason: (.+)', report)\n",
    "        admitting_diagnosis = re.search(r'Admitting Diagnosis: (.+)', report)\n",
    "        medical_condition = re.search(r'MEDICAL CONDITION:\\n\\s+(.+)', report)\n",
    "        final_report = re.search(r'FINAL REPORT\\n\\s+(.+)', report, re.DOTALL)\n",
    "\n",
    "        # Building the report string\n",
    "        report_string = \"\"\n",
    "        if report_id:\n",
    "            report_string += f\"Report ID: {report_id.group(1)}\\n\"\n",
    "        if type_of_exam:\n",
    "            report_string += f\"Type of Exam: {type_of_exam.group(1)}\\n\"\n",
    "        if clip_number:\n",
    "            report_string += f\"Clip Number: {clip_number.group(1)}\\n\"\n",
    "        if reason_for_exam:\n",
    "            report_string += f\"Reason for Exam: {reason_for_exam.group(1)}\\n\"\n",
    "        if admitting_diagnosis:\n",
    "            report_string += f\"Admitting Diagnosis: {admitting_diagnosis.group(1)}\\n\"\n",
    "        if medical_condition:\n",
    "            report_string += f\"Medical Condition: {medical_condition.group(1)}\\n\"\n",
    "        if final_report:\n",
    "            report_string += f\"Final Report: {final_report.group(1)}\\n\"\n",
    "\n",
    "        report_string = report_string.strip()\n",
    "        \n",
    "        # Add report to the extracted reports if it's not a duplicate\n",
    "        if report_string not in seen_reports:\n",
    "            extracted_reports.append(report_string)\n",
    "            seen_reports.add(report_string)\n",
    "        \n",
    "    # concatenate all the reports into a single string\n",
    "    return \"\\n\\n\".join(extracted_reports)\n",
    "\n",
    "# apply the function to the TEXT column\n",
    "tqdm.pandas()\n",
    "df[\"Cleaned_TEXT\"] = df[\"TEXT\"].progress_apply(parse_medical_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty TEXT\n",
    "df = df[df[\"Cleaned_TEXT\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3304 entries, 0 to 3557\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   VAP           3304 non-null   int64 \n",
      " 1   TEXT          3304 non-null   object\n",
      " 2   Cleaned_TEXT  3304 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3304/3304 [00:03<00:00, 1019.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# import gensim.downloader as api\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Load a pre-trained word2vec model (this is just an example model)\n",
    "# embedder = api.load(\"glove-wiki-gigaword-100\")  # This model has 100-dimensional embeddings\n",
    "\n",
    "# def get_word_embeddings(sample):\n",
    "#     words = sample.split()\n",
    "#     embeddings = []\n",
    "#     for word in words:\n",
    "#         if word in embedder.key_to_index:\n",
    "#             embeddings.append(embedder[word])\n",
    "#         else:\n",
    "#             continue\n",
    "#     return np.array(embeddings)\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Load your pre-trained word2vec model\n",
    "embedder = gensim.models.KeyedVectors.load_word2vec_format(\"pubmed_mesh_test.bin\", binary=True)\n",
    "\n",
    "def get_word_embeddings(sample):\n",
    "    words = sample.split()\n",
    "    # Create a list to hold embeddings\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in embedder.key_to_index:\n",
    "            # Add the word embedding to the list\n",
    "            embeddings.append(embedder[word])\n",
    "        else:\n",
    "            continue\n",
    "    return np.array(embeddings)\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"EMBEDDINGS\"] = df[\"Cleaned_TEXT\"].progress_apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty EMBEDDINGS\n",
    "df = df[df[\"EMBEDDINGS\"].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMBEDDINGS\n",
       "(121, 200)     14\n",
       "(302, 200)     13\n",
       "(142, 200)     12\n",
       "(62, 200)      12\n",
       "(54, 200)      10\n",
       "               ..\n",
       "(1347, 200)     1\n",
       "(1806, 200)     1\n",
       "(1926, 200)     1\n",
       "(3572, 200)     1\n",
       "(863, 200)      1\n",
       "Name: count, Length: 1505, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"EMBEDDINGS\"].apply(lambda x: x.shape).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart of Label\n",
    "df['VAP'].value_counts().plot.pie(autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # Separate the embeddings and labels\n",
    "        embeddings, labels = zip(*batch)\n",
    "\n",
    "        # Convert embeddings from numpy arrays to torch tensors\n",
    "        embeddings_tensor = [torch.tensor(e, dtype=torch.float32) for e in embeddings]\n",
    "\n",
    "        # Pad the embeddings to the same length\n",
    "        embeddings_padded = pad_sequence(embeddings_tensor, batch_first=True, padding_value=0)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return embeddings_padded, labels_tensor\n",
    "\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, num_classes, dim_feedforward=2048, num_layers=4, num_conv_layers=2, conv_kernel_size=3):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels=embedding_size, \n",
    "                                                    out_channels=embedding_size, \n",
    "                                                    kernel_size=conv_kernel_size, \n",
    "                                                    padding=conv_kernel_size // 2) for _ in range(num_conv_layers)])\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_size, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=dim_feedforward\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Additional fully connected layers\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size * 2)\n",
    "        self.fc2 = nn.Linear(embedding_size * 2, embedding_size)\n",
    "\n",
    "        # Final classifier layer\n",
    "        self.classifier = nn.Linear(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose x to match the input requirement of conv1d: [batch_size, channels, sequence_length]\n",
    "        x = x.transpose(1, 2)  # Transpose the dimensions to [batch_size, embedding_size, sequence_length]\n",
    "\n",
    "        # Pass through convolutional layers\n",
    "        for conv in self.conv_layers:\n",
    "            x = F.relu(conv(x))\n",
    "\n",
    "        # Transpose back to [batch_size, sequence_length, embedding_size] for the transformer encoder\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Pass through transformer encoder layers\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Aggregate over the sequence dimension\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.2)  # Dropout for regularization\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Classification\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings and labels to a suitable format\n",
    "embeddings = np.array(df['EMBEDDINGS'].values)\n",
    "labels = df['VAP'].values\n",
    "\n",
    "# Split data into training and validation sets (80-20 split)\n",
    "embeddings_train, embeddings_val, labels_train, labels_val = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom datasets for training and validation\n",
    "train_dataset = MedicalDataset(embeddings_train, labels_train)\n",
    "val_dataset = MedicalDataset(embeddings_val, labels_val)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=MedicalDataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=MedicalDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Training Loss: 0.6864581836455158\n",
      "Validation Accuracy: 0.832572298325723\n",
      "Epoch 1, Average Training Loss: 0.6120394558576325\n",
      "Validation Accuracy: 0.832572298325723\n",
      "Epoch 2, Average Training Loss: 0.6606506244418626\n",
      "Validation Accuracy: 0.832572298325723\n",
      "Epoch 3, Average Training Loss: 0.6106339289354589\n",
      "Validation Accuracy: 0.832572298325723\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):  \u001b[39m# number of epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mfor\u001b[39;00m embeddings_batch, labels_batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39m# Send data to device (GPU or CPU)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         embeddings_batch \u001b[39m=\u001b[39m embeddings_batch\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         labels_batch \u001b[39m=\u001b[39m labels_batch\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 626\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_profile_name):\n\u001b[1;32m    627\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/profiler.py:627\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[39m# TODO: TorchScript ignores standard type annotation here\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[39m# self.record: Optional[\"torch.classes.profiler._RecordFunction\"] = None\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(\n\u001b[0;32m--> 627\u001b[0m     Optional[\u001b[39m\"\u001b[39;49m\u001b[39mtorch.classes.profiler._RecordFunction\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/typing.py:274\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    273\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[39mreturn\u001b[39;00m cached(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    275\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_size = embedder.vector_size  # Size of a single word embedding\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_classes = 2  # Number of output classes\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_conv_layers = 2  # Number of convolutional layers\n",
    "model = Transformer(embedding_size, num_heads, num_classes, num_layers=num_layers, num_conv_layers=num_conv_layers)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.01  \n",
    "weight_decay = 0.001  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 'labels' is an array of all the labels in your training dataset\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels_train), y=labels_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(20):  # number of epochs\n",
    "    total_loss = 0\n",
    "    for embeddings_batch, labels_batch in train_loader:\n",
    "        # Send data to device (GPU or CPU)\n",
    "        embeddings_batch = embeddings_batch.to(device).float()\n",
    "        labels_batch = labels_batch.to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings_batch)\n",
    "\n",
    "        # Compute weighted loss\n",
    "        loss = F.cross_entropy(outputs, labels_batch, weight=class_weights)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}, Average Training Loss: {avg_loss}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for embeddings_batch, labels_batch in val_loader:\n",
    "            embeddings_batch = torch.tensor(embeddings_batch).to(device).float()\n",
    "            labels_batch = torch.tensor(labels_batch).to(device).long()\n",
    "\n",
    "            outputs = model(embeddings_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels_batch.size(0)\n",
    "            correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
