{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty text\n",
    "data = data[data['TEXT'] != '']\n",
    "\n",
    "# reindex the DataFrame\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a new dataframe that contains as much data from each class as the number of rows in the class with the least rows\n",
    "data_balanced = pd.concat([data[data['VAP'] == 0].sample(data[data['VAP'] == 1].shape[0]), data[data['VAP'] == 1]])\n",
    "data = data_balanced\n",
    "# pie chart of Label\n",
    "data['VAP'].value_counts().plot.pie(autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['TEXT'].tolist(), data['VAP'].tolist(), test_size=0.2, random_state=42, stratify=data['VAP'].tolist())\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "\n",
    "#tokenizer = transformers.BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', do_lower_case=True)\n",
    "#bert_model = transformers.BertModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "SEQ_LEN = 128\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "learning_rate = 1e-3 # Controls how large a step is taken when updating model weights during training.\n",
    "steps_per_epoch = 20\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(text, chunk_size=128, overlap=32):\n",
    "    words = text.split()\n",
    "    if len(words) // chunk_size > 0:\n",
    "        n_chunks = len(words) // chunk_size\n",
    "    else: \n",
    "        n_chunks = 1\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size\n",
    "        chunks.append(\" \".join(words[start:end]))\n",
    "        if i != n_chunks - 1:  # Add overlap to all but the last chunk\n",
    "            end += overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "X_train_chunks = [get_split(text) for text in X_train]\n",
    "X_val_chunks = [get_split(text) for text in X_val]\n",
    "X_test_chunks = [get_split(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = [chunk for text_chunks in X_train_chunks for chunk in text_chunks]\n",
    "X_val_flattened = [chunk for text_chunks in X_val_chunks for chunk in text_chunks]\n",
    "X_test_flattened = [chunk for text_chunks in X_test_chunks for chunk in text_chunks]\n",
    "\n",
    "X_encoding_train = tokenizer.batch_encode_plus(\n",
    "  X_train_flattened,\n",
    "  max_length=SEQ_LEN,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=True,\n",
    "  truncation=True,\n",
    "  padding='longest',\n",
    "  return_attention_mask=True,\n",
    ")\n",
    "\n",
    "X_encoding_val = tokenizer.batch_encode_plus(\n",
    "    X_val_flattened,\n",
    "    max_length=SEQ_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "X_encoding_test = tokenizer.batch_encode_plus(\n",
    "    X_test_flattened,\n",
    "    max_length=SEQ_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoding_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training data:  5028\n",
      "length of validation data:  521\n",
      "length of test data:  585\n"
     ]
    }
   ],
   "source": [
    "X_train_input_ids = torch.tensor(X_encoding_train['input_ids'])\n",
    "X_train_attention_mask = torch.tensor(X_encoding_train['attention_mask'])\n",
    "X_train_token_type_ids = torch.tensor(X_encoding_train['token_type_ids'])\n",
    "\n",
    "X_val_input_ids = torch.tensor(X_encoding_val['input_ids'])\n",
    "X_val_attention_mask = torch.tensor(X_encoding_val['attention_mask'])\n",
    "X_val_token_type_ids = torch.tensor(X_encoding_val['token_type_ids'])\n",
    "\n",
    "X_test_input_ids = torch.tensor(X_encoding_test['input_ids'])\n",
    "X_test_attention_mask = torch.tensor(X_encoding_test['attention_mask'])\n",
    "X_test_token_type_ids = torch.tensor(X_encoding_test['token_type_ids'])\n",
    "\n",
    "num_chunks_per_train_text = [len(chunks) for chunks in X_train_chunks]\n",
    "num_chunks_per_val_text = [len(chunks) for chunks in X_val_chunks]\n",
    "num_chunks_per_test_text = [len(chunks) for chunks in X_test_chunks]\n",
    "\n",
    "y_train_chunks = torch.tensor([label for label, n_chunks in zip(y_train, num_chunks_per_train_text) for _ in range(n_chunks)])\n",
    "y_val_chunks = torch.tensor([label for label, n_chunks in zip(y_val, num_chunks_per_val_text) for _ in range(n_chunks)])\n",
    "y_test_chunks = torch.tensor([label for label, n_chunks in zip(y_test, num_chunks_per_test_text) for _ in range(n_chunks)])\n",
    "\n",
    "\n",
    "print(\"length of training data: \", len(X_train_input_ids))\n",
    "print(\"length of validation data: \", len(X_val_input_ids))\n",
    "print(\"length of test data: \", len(X_test_input_ids))\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train_input_ids, X_train_attention_mask, X_train_token_type_ids, y_train_chunks)\n",
    "val_data = torch.utils.data.TensorDataset(X_val_input_ids, X_val_attention_mask, X_val_token_type_ids, y_val_chunks)\n",
    "test_data = torch.utils.data.TensorDataset(X_test_input_ids, X_test_attention_mask, X_test_token_type_ids, y_test_chunks)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_classes, freeze_bert=False):\n",
    "        \n",
    "        super(TransformerModel,self).__init__()\n",
    "        # Instantiating BERT model object\n",
    "        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n",
    "        \n",
    "        # Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        self.bert_drop_1 = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n",
    "        self.bn = nn.BatchNorm1d(768) # (768)\n",
    "        self.bert_drop_2 = nn.Dropout(0.25)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,2)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        output = self.bert_drop_1(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.bn(output)\n",
    "        output = self.bert_drop_2(output)\n",
    "        output = self.out(output)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "model = TransformerModel(data['VAP'].nunique(), freeze_bert=False)\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [{'params': [p for n, p in param_optimizer \n",
    "                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n",
    "                        {'params': [p for n, p in param_optimizer \n",
    "                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n",
    "\n",
    "#optimizer \n",
    "optimizer = AdamW(optimizer_parameters, lr=learning_rate)\n",
    "steps = steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 3,\n",
    "    num_training_steps = steps\n",
    ")\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Step 30/315 Loss 1.1546:  10%|▉         | 30/315 [00:08<01:24,  3.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/VPA-Disease-Prediction-from-Medical-Record/test.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:466\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    462\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    464\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[1;32m    467\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    468\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_accuracy = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('-' * 10)\n",
    "    \n",
    "    model = model.train()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, batch in progress_bar:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2],\n",
    "                  'labels': batch[3]}\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'])\n",
    "        \n",
    "        loss = criteria(outputs, inputs['labels'])\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = inputs['labels'].to('cpu').numpy()\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        all_labels.extend(labels)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "        progress_bar.set_description(f'Epoch {epoch + 1}/{epochs} Step {step+1}/{len(train_loader)} Loss {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss}\")\n",
    "    print(f\"Training Accuracy: {accuracy}\")\n",
    "    print(f\"Training F1 Score: {f1}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Reset total_loss for the next epoch\n",
    "    total_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
